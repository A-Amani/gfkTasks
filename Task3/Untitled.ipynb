{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import abc\n",
    "from nltk import pos_tag\n",
    "import pandas.core.series as pdSeries\n",
    "\n",
    "from config import *\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DataReader():\n",
    "    def __init__(self):\n",
    "        self.mandatory_cols = config[\"mandatory_cols\"]\n",
    "        self.native_cols = config[\"data_source_native_cols\"]\n",
    "        self.data_source_path = config[\"source_data\"]\n",
    "        self.seperator = config[\"seperator\"]\n",
    "\n",
    "    def retrieve_data_source(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        pdf = self.read_dataFile()\n",
    "        self.validate_datasource(pdf)\n",
    "        return pdf\n",
    "\n",
    "\n",
    "    def read_dataFile(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pdf = pd.read_csv(self.data_source_path, delimiter = self.seperator, names=self.native_cols)\n",
    "        except Exception:\n",
    "            raise Exception(\"cannot read the data_handling source!\")\n",
    "        return pdf\n",
    "\n",
    "    def validate_datasource(self, pdf:  pd.DataFrame):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if set(self.mandatory_cols).issubset(set(pdf.columns)):\n",
    "            pass\n",
    "        else:\n",
    "            logger.ERROR(\"!!! Mandatory cols are not present in the data_handling source provided !!\")\n",
    "            raise Exception(\"Mandatory cols are not present in the data_handling source provided \")\n",
    "\n",
    "\n",
    "\n",
    "class FeatEngineering(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.run_mode = config[\"run_mode\"]\n",
    "\n",
    "        #self.punctuation = set(string.punctuation)\n",
    "        #self.stop_words = set(stopwords.words('english'))\n",
    "        #self.lambda_not_number = lambda x: not any(re.match(\"^\\d*$\", x.replace(punc, \"\")) for punc in self.punctuation)\n",
    "        #self.lambda_no_punc = lambda x: self.no_punc(x)\n",
    "        #self.accepted_POS = [\"NN\", \"VB\", \"CD\"]\n",
    "\n",
    "        self.skewness_threshold = config[\"skewness_threshold\"]\n",
    "        self.fill_nan_approach = config[\"fill_nan_approach\"]\n",
    "        self.feature_cols = config[\"feature_col\"]\n",
    "        self.target_col = config[\"native_target_col\"]\n",
    "        self.final_feature_col = config['final_feature_col']\n",
    "        self.pickle_folder = config[\"model_folder\"]\n",
    "        if self.run_mode == RUN_MODE.TRAIN.value:\n",
    "            self.test_train_split = config[\"test_train_split_ratio\"]\n",
    "        logger.info(f\"** doing feature engineering\")\n",
    "\n",
    "\n",
    "\n",
    "    def do_feature_engineering(self, pdf: pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info(\"** start feature engineering! **\")\n",
    "        if self.run_mode == RUN_MODE.TRAIN.value:\n",
    "            self.checkSkewness(pdf=pdf)\n",
    "            pdf = self.dealWithNan(pdf=pdf)\n",
    "            pdf = self.uniteFeatureColumns(pdf=pdf)\n",
    "            x_train, y_train, x_test, y_test = self.split_test_train(pdf=pdf)\n",
    "            res = [x_train, y_train, x_test, y_test]\n",
    "        elif self.run_mode == RUN_MODE.PREDICT.value:\n",
    "            res = x\n",
    "        else:\n",
    "            raise Exception(\" Selected RUN_MODE is not correct!...\")\n",
    "\n",
    "\n",
    "        logger.info(f\"** FINISHED feature engineering!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def checkSkewness(self, pdf: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        To balance the target categories. after check the skewness of the target column, downsample\n",
    "         is performed to balance the data_handling.\n",
    "        \"\"\"\n",
    "        pdf_count   = pdf.groupby([self.target_col]).size().to_frame(name=\"counts\").reset_index()\n",
    "        skewness    = skew(pdf_count.values[:, 1])\n",
    "        if_skewed \t= True if abs(skewness) > self.skewness_threshold else False\n",
    "\n",
    "        if if_skewed:\n",
    "            logger.warning(\"** Data is skewed. down-sampling or up-sampling is suggested\")\n",
    "\n",
    "\n",
    "    def dealWithNan(self, pdf: pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        if self.fill_nan_approach == \"empty_string\":\n",
    "            pdf = pdf.fillna(value='')\n",
    "        elif self.fill_nan_approach == \"drop\":\n",
    "            pdf = pdf.dropna()\n",
    "        else: \n",
    "            logger.ERROR(\"** selected Nan handdling approach is not valid\")\n",
    "        return pdf\n",
    "\n",
    "\n",
    "    def uniteFeatureColumns(self, pdf: pd.DataFrame):\n",
    "        pdf[self.final_feature_col] = pdf[self.feature_cols].agg(' '.join, axis=1)\n",
    "        return pdf\n",
    "\n",
    "\n",
    "    def split_test_train(self, pdf: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Returns: x_train, y_train, x_test, y_test\n",
    "        \"\"\"\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(pdf[[self.final_feature_col]], pdf[self.target_col], test_size=self.test_train_split, random_state=0)\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.run_mode = config[\"run_mode\"]\n",
    "        self.model_folder = config[\"model_folder\"]\n",
    "        self.model_selection = config[\"model_selection\"]\n",
    "        #self.model_parameters = config['model_patameters']\n",
    "        self.model_file_name = f\"{self.model_folder}/model.pkl\"\n",
    "        self.final_feature_col = config['final_feature_col']\n",
    "\n",
    "        self.model = None\n",
    "        \n",
    "        if self.run_mode == RUN_MODE.PREDICT.value:\n",
    "            self.prediction_col = config[\"prediction_col\"]\n",
    "            self.output_file_name = config[\"output_file_name\"]\n",
    "            with open(f\"{self.model_folder}/{config['label_encoder']}.pkl\", \"rb\") as fp:\n",
    "                self.le = pickle.load(fp)\n",
    "        logger.info(f\"** do model {self.run_mode}, active model is {self.model_selection} !!\")\n",
    "\n",
    "\n",
    "    def construct_preProcessing_pipline(self):\n",
    "        descriptive_features_pipeline = Pipeline(steps=\n",
    "            [\n",
    "                ('CountVectorizer', CountVectorizer()),\n",
    "                ('Tfidf', TfidfTransformer())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessing_pipeline = ColumnTransformer(transformers=\n",
    "            [\n",
    "                ('num',descriptive_features_pipeline, self.final_feature_col)\n",
    "            ]\n",
    "        )\n",
    "        return preprocessing_pipeline\n",
    "\n",
    "    def construct_classifier_pipline(self):\n",
    "        if self.model_selection == 'svm': \n",
    "            classifier_pipline = SGDClassifier()\n",
    "        elif self.model_selection == 'naive_bayes': \n",
    "            classifier_pipline = MultinomialNB()\n",
    "        elif self.model_selection == 'knn': \n",
    "            classifier_pipline = KNeighborsClassifier()\n",
    "        elif self.model_selection == 'logistic_regression': \n",
    "            classifier_pipline = LogisticRegression()\n",
    "\n",
    "        return classifier_pipline\n",
    "\n",
    "    def construct_model_pipline(self, preprocessing_pipeline, classifier_pipline):\n",
    "        pipe = Pipeline(steps=\n",
    "            [\n",
    "                ('preprocessor', preprocessing_pipeline),\n",
    "                ('classifier', classifier_pipline)\n",
    "            ]\n",
    "        )\n",
    "        return pipe\n",
    "\n",
    "    \n",
    "    def run(self, data):\n",
    "        if self.run_mode == RUN_MODE.TRAIN.value:\n",
    "            x_train, y_train, x_test, y_test = data\n",
    "            print(y_test.head())\n",
    "            pp   = self.construct_preProcessing_pipline()\n",
    "            clf  = self.construct_classifier_pipline()\n",
    "            pipe = self.construct_model_pipline(pp,clf)\n",
    "            self.model = pipe.fit(x_train, y_train)\n",
    "            Model.save_model(model=self.model, file_name=self.model_file_name)\n",
    "            logger.info(f\"saved model: {self.model_selection} to {self.model_file_name}\")\n",
    "\n",
    "            self.evaluate_model(x_test, y_test, self.model)\n",
    "                    \n",
    "        if self.run_mode == RUN_MODE.PREDICT.value:\n",
    "            self.model = Model.load_model(model=self.model, file_name=self.model_file_name)\n",
    "            x = data\n",
    "            y = self.model.predict(x)\n",
    "            logger.info(f\"predicted values are: {y}\")\n",
    "\n",
    "            \n",
    "    \n",
    "                \n",
    "    def evaluate_model(self, x_test: pdSeries.Series, y_test: pdSeries.Series, pipe:Pipeline):\n",
    "        predicted = pipe.predict(x_test)\n",
    "        logger.info(\"model accuracy is: %.3f \\n\" % pipe.score(x_test, y_test) )\n",
    "        logger.info(metrics.classification_report(y_test, predicted, target_names=y_test.unique()))\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model, file_name:str):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if file_name != \"\":\n",
    "            with open(file_name, 'wb') as fp:\n",
    "                pickle.dump(model, fp)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_name: str):\n",
    "        try:\n",
    "            with open(file_name, 'rb') as fp:\n",
    "                return pickle.load(fp)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"model doesn't exist! need to train first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:** doing feature engineering\n",
      "INFO:__main__:** start feature engineering! **\n",
      "WARNING:__main__:** Data is skewed. down-sampling or up-sampling is suggested\n",
      "INFO:__main__:** FINISHED feature engineering!\n",
      "INFO:__main__:** do model train, active model is svm !!\n",
      "INFO:__main__:saved model: svm to saved_models/model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3069           BICYCLES\n",
      "1675     CONTACT LENSES\n",
      "6363    WASHINGMACHINES\n",
      "543     WASHINGMACHINES\n",
      "3214    WASHINGMACHINES\n",
      "Name: productgroup, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:model accuracy is: 0.998 \n",
      "\n",
      "INFO:__main__:                 precision    recall  f1-score   support\n",
      "\n",
      "       BICYCLES       1.00      1.00      1.00       611\n",
      " CONTACT LENSES       1.00      1.00      1.00       626\n",
      "WASHINGMACHINES       1.00      1.00      1.00       602\n",
      "     USB MEMORY       1.00      1.00      1.00       562\n",
      "\n",
      "       accuracy                           1.00      2401\n",
      "      macro avg       1.00      1.00      1.00      2401\n",
      "   weighted avg       1.00      1.00      1.00      2401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataReader()\n",
    "data = data_handler.retrieve_data_source()\n",
    "\n",
    "feature_handler = FeatEngineering()\n",
    "pdf = featureObj.do_feature_engineering(data)\n",
    "\n",
    "model = Model()\n",
    "model.run(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
